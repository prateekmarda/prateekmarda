{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4mdNbSFuXpY"
      },
      "source": [
        "## **Project Idea: Predictive Supply Chain and Inventory Management System**\n",
        "\n",
        "###● Description: Develop an AI system that predicts inventory demand, monitors supply chain disruptions, and provides recommendations to optimize stock levels.\n",
        "\n",
        "● Key Features:\n",
        "\n",
        "    o LLM Fine-Tuning on Supply Chain Data: Fine-tune an LLM on historical supply chain data, logistics reports & inventory management systems.\n",
        "    o RAGs for Supply Chain Insights: Use RAGs to fetch real-time supply chain data, news, and market trends.\n",
        "    o Agent for Optimization: Predict inventory requirements and optimize supply chain routes and stock levels.\n",
        "\n",
        "● Steps:\n",
        "\n",
        "    o Collect datasets of supply chain logs, inventory data, and product demand forecasts.\n",
        "    o Fine-tune the LLM to understand supply chain operations and logistics.\n",
        "    o Implement RAGs to fetch real-time data on inventory, suppliers, and logistics disruptions.\n",
        "    o Build an agent that provides real-time supply chain optimization and recommendations.\n",
        "    o Evaluate the system’s performance using real-world supply chain scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqkYA-4litOU"
      },
      "source": [
        "Install Unsloth opensource fine tuning and use FastLanguageMode. In this project i have used PEFT fine tuning method based on LoRa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqGaDe6_i1cS"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7LDjZ7rjCze"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "max_seq_length = 1024\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_compute_dtype = torch.float16,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    llm_int8_enable_fp32_cpu_offload = True, # Crucial for allowing 32-bit CPU offload\n",
        ")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    device_map = \"auto\",\n",
        "    quantization_config = quantization_config,\n",
        "    dtype = torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBThM9-vrLqu"
      },
      "source": [
        "I will now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train <1% of all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW3M9MxCrVzL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsXz42b3sJGI"
      },
      "source": [
        "Data Prep Stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsV7K_57r7Lz"
      },
      "outputs": [],
      "source": [
        "# Loading sample supply chain data set from HuggingFace for demo purposes in real life would integrate the company's historic data for better grounding\n",
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-4\",\n",
        ")\n",
        "\n",
        "ds = load_dataset(\"alalfi/SupplyChainDataset\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYsGydE4s_kB"
      },
      "source": [
        "I will now use standardize_sharegpt to convert ShareGPT style datasets into HuggingFace generic format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vypBgjCgtTWX"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Redefine tokenizer to ensure the chat template is applied\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-4\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for i in range(len(examples['Product_Name'])):\n",
        "        product_name = examples['Product_Name'][i]\n",
        "        # Assuming 'Order_Item_Quantity' is a suitable replacement for 'Stock Quantity'\n",
        "        stock_quantity = examples['Order_Item_Quantity'][i]\n",
        "        # Assuming 'Days_for_shipping_(real)' is a suitable replacement for 'Lead Time (days)'\n",
        "        lead_time = examples['Days_for_shipping_(real)'][i]\n",
        "        # Assuming delivery status for getting risks of late deliveries as an example\n",
        "        delivery_status = examples['Delivery_Status'][i]\n",
        "        # Add new columns for more comprehensive insights\n",
        "        late_delivery_risk = examples['Late_delivery_risk'][i]\n",
        "        order_status = examples['Order_Status'][i]\n",
        "        shipping_mode = examples['Shipping_Mode'][i]\n",
        "        sales_amount = examples['Sales'][i]\n",
        "\n",
        "        #some allied fields for extra information:\n",
        "        customer_city = examples['Customer_City'][i]\n",
        "        customer_country = examples['Customer_Country'][i]\n",
        "        customer_segment = examples['Customer_Segment'][i]\n",
        "        department_name = examples['Department_Name'][i]\n",
        "        product_price = examples['Product_Price'][i]\n",
        "        product_description = examples['Product_Description'][i]\n",
        "        # Assuming 'Category_Name' is a suitable replacement for 'Category'\n",
        "        category = examples['Category_Name'][i]\n",
        "\n",
        "        # Create a simple prompt-response structure for fine-tuning, this can be customized based on what you want the LLM to learn\n",
        "        conversation = [\n",
        "            {\"role\": \"user\", \"content\": f\"What are the key supply chain details for product '{product_name}'?\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Product Name: {product_name}, Category: {category}, Stock Quantity: {stock_quantity}, Lead Time: {lead_time} days, Delivery Status: {delivery_status}, Late Delivery Risk: {late_delivery_risk}, Order Status: {order_status}, Shipping Mode: {shipping_mode}, Sales: {sales_amount}.\"}\n",
        "        ]\n",
        "        texts.append(tokenizer.apply_chat_template(\n",
        "            conversation, tokenize=False, add_generation_prompt=False\n",
        "        ))\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Apply the custom formatting function directly to the dataset\n",
        "dataset = ds.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    remove_columns=ds.column_names # Remove original columns if not needed after text generation\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "b1CK_1Kjl3qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8, # 8 for more stable gradients\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2,\n",
        "        max_steps = 30,\n",
        "        learning_rate = 1e-5,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.1,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        fp16 = True,\n",
        "        bf16 = False,\n",
        "        max_grad_norm = 0.3, # Added gradient clipping to prevent exploding gradients\n",
        "    ),\n",
        ")\n",
        "# The set_format call is moved here to ensure labels are tensors before train_on_responses_only\n",
        "trainer.train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"text\"])"
      ],
      "metadata": {
        "id": "NLP5YJXql5TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "vxw70b_XmgQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"input_ids\"]])"
      ],
      "metadata": {
        "id": "e-VqskMNmoRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show System Stats"
      ],
      "metadata": {
        "id": "ZK1Xm8DIm1WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "eynaltEjm2zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print Trainer Stats\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "xop4jknHnDjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f33ba81"
      },
      "source": [
        "display(trainer_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print Time Stats"
      ],
      "metadata": {
        "id": "PGYVlBDboryd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "xmTz9ChmouXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferencing the model"
      ],
      "metadata": {
        "id": "jn4Ghbb7pp6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-4\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    #Comment one to understand the model response behaviour for answer to the question which it doesn't completely know the answer and for one which it may know\n",
        "    #{\"role\": \"user\", \"content\": \"What are the best strategies to optimize inventory stock levels and reduce holding costs?\"}, #Model doesn't accurately know the answer\n",
        "    {\"role\": \"user\", \"content\": \"What products are on risk of late delivery?\"}, #Model may accurately know the answer from trining data.\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs, max_new_tokens = 150, use_cache = True, temperature = 1.5, min_p = 0.1\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "4OQTziWUpscy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Which customer is impacted by late deliveries?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(\n",
        "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 100,\n",
        "    use_cache = False, temperature = 1.5, min_p = 0.1\n",
        ")"
      ],
      "metadata": {
        "id": "tFbsm8n2Utu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save pretrained model for later use"
      ],
      "metadata": {
        "id": "8rEjUQxDVL1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "model.push_to_hub(\"prateekmarda/lora_model\", token = \"\") # Online saving , token has been intentionally removed pls provide your HF access token or other allied like so here\n",
        "tokenizer.push_to_hub(\"prateekmarda/lora_model\", token = \"\") # Online saving , token has been intentionally removed pls provide your HF access token or other allied like so here"
      ],
      "metadata": {
        "id": "qz4kHsDbVOcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This completes Part 1 of the project which is Finetuning an LLM on sample supply chain data"
      ],
      "metadata": {
        "id": "hVxQR44gBOS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip GGI5A_notebook_archive_S12456.zip GGI5A_S12456_Capstone_SCM_Part1_Finetuning_LLM.ipynb"
      ],
      "metadata": {
        "id": "LRCdu5qOJnEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d064f7eb"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}